---
title: Restricting Algorithmic Propagation of Misnformation
author: Lucas LaValva
date: November 30, 2021
output: pdf_document
geometry: "margin=1in"
header-includes:
    - \usepackage{setspace}
    - \doublespacing
    - \usepackage{hanging}
    - \usepackage{fancyhdr}
    - \pagestyle{fancy}
    - \lhead{Lucas LaValva}
    - \chead{Restricting Algorithmic Propagation of Misnformation}
    - \rhead{Sociology Seminar}
bibliography: [references.bib]
---

# Abstract

The spread of misinformation and "fake news" in social media has been a major source of concern for a large number of vocal members of society in recent years. It is often stated that this problem has worsened as the internet has become a larger part of daily life due to the way that algorithms encourage _all_ information to spread, regardless of its reliability. This proposal addresses the causes of this algorithmic propagation of misinformation and proposes an experiment that may be performed in order to learn more about a new method for its mitigation. In this new method, the idea will be explored that slowing the propagation of all news and information on social media may encourage users to naturally filter out unreliable sources. This hypothesis is backed by a large body of current research on internet-based misinformation.

# Introduction

Since the advent of the internet, communication among members of society has undergone a massive transformation. Gone are the days when information was expensive to share, so only the most important and potentially impactful news was condensed before it was shipped long distances. Most of humanity now belongs to a society in which any member has the power to reach every other person, regardless of distance and time. This has been a revolutionary improvement for countless individuals, as it is now nearly impossible to silence the voices of people who feel that they need to be heard. While this freedom is advantageous for workers who are mistreated and for whistleblowers who have important information which has been suppressed from the public, it is also extremely helpful for people who want to spread news or information with malicious political or capitalistic intent. Many professional news sources and academics have begun to wonder whether the freedom that the internet generates for people from all levels of education and walks of life to anonymously speak from the same podium has resulted in unintended negative consequences. The consequence that is most often discussed is the proliferation of "fake news" and other misinformation on the internet.

A very large body of politicians, news anchors, academics, and activists have been voicing negative opinions about rampant misinformation for many years, but public knowledge of the problem seems to have grown exponentially since the United States presidential election in 2016, as large political campaigns were built around the presence of "fake news" that has supposedly made its way into mainstream media. Because of the large amount of attention that this topic has received, social media companies and academic institutions alike are feverishly seeking methods that may be used to mitigate the problem. Hundreds of research papers are produced every year which study specific aspects of internet misinformation, such as the potential reasons why it is appealing to users of social media, the way that a topic propagates throughout the internet, and the detection of misleading news with the use of algorithms. One topic of discussion that seems not to have received as much attention is the way that algorithms encourage certain posts and bodies of information to spread.

# Literature Review

## Identification of Misinformation

A study by @Wu_2019_MisinformationInSocialMedia examines misinformation in social media and discusses the potential for detecting it with machine learning algorithms. The issue they feel is most important is that the definition of misinformation varies wildly across current literature. It always describes the information that is fake or inaccurate, but this definition is far too broad to be studied extensively. As such, they define eleven categories and subcategories of misinformation that can each be studied individually. These eleven definitions may each be an individual source of extensive study, and therefore are deserving of attention on their own. These forms are organized as follows:

- __Unintentionally spread misinformation__ is misinformation that is not meant to deceive the people to whom it is sent, and recipients and senders alike believe in its value purely because it comes from a source that they trust such as family or an influential social media personality.
- __Intentionally spread misinformation__ is misinformation that comes from a source that intends to deceive its recipients.  This type of misinformation generally comes from organized groups such as political activists and public resources departments of companies who have built an agenda that they are trying to follow.
  - __Urban legend__ is misinformation spread intentionally that is centered around localized fictional stories. This misinformation is often generated as entertainment and is used for that purpose before it acquires a following of people who believe the stories.
  - __Fake news__ is misinformation that is provided in the same format as mainstream news articles. This is often a result of propaganda campaigns.
- __Unverified information__ can be true or false, but it is not uncommon for sources of media to spread information throughout the media as though it is true without ensuring that it has been verified so the authors decided to include it as a form of misinformation.
  - __Rumor__ is a type of unverified information that has been spread mostly by people who are not generally involved with mainstream media.
- __Crowdturfing__ is a method for artificially inflating perceived support for a certain piece of information through the use of a faked grassroots campaign from an organized central source. This type of misinformation may be intentionally spread with malicious intent, or it may be unverified information.
- __Spam__ is information that may be verified or unverified, but it is sent to its recipients in large waves to overwhelm them.
- __Troll__ is misinformation that is built to widen the gap between two sets of beliefs instead of advocating for a single side. This is often meant to build hatred between groups.
- __Hate speech__ is misinformation that is designed to marginalize and abuse specific people or groups of people. This is said to have been rampant during the 2016 election [@Wu_2019_MisinformationInSocialMedia]
  - __Cyberbulling__ is a form of hate speech that targets a specific person or a small group, with the expectation that they will be emotionally harmed.

Within these forms of misinformation, some important variables are brought up continuously. The most frequent variables are the intention of deceit and the verification of information.

After defining categories of misinformation, the researchers determined that they intended to focus on false information which is intentionally and maliciously spread in social networks. The two main types of attacks in social media are **manipulation of networks**, in which fake accounts attach themselves to preexisting social networks or form their own, and **manipulation of content**, in which they copy large portions of information from real accounts and then introduce illegitimate information. They went on to discuss each of these types of attacks in detail and described ways in which spam accounts could potentially be detected automatically. Four methods of detection were proposed; an illegitimate post may be identified based on the contents of the post, the context in which it was posted, the patterns with which the post propagated throughout the platform, or before it becomes viral using a combination of these techniques. Misinformation in social media posts is often contextualized with very little supporting data or labels, so if this lack of data can be detected automatically it may be possible for social media platforms to stop the viral spread of a topic that may be misinformed before it becomes immensely popular.

@Wu_2019_MisinformationInSocialMedia spend the rest of their research paper discussing variables that should be considered by machine learning algorithms built to identify malicious sources of misinformation, and methods that may be used to find organized groups. The approach that these researchers have taken, in which the goal is to find sources of misinformation instead of misinformation itself, is fairly unique in this research field. A large majority of research papers that are designed to identify misinformation take a more granular approach, as they focus on the topics of misinformation themselves or on the specific articles that are misinformed. 

In order to track misinformation at a large scale, researchers are tasked with the challenge of finding a way to quickly identify the position of an article or social media post. Because of the broad nature of this question, @Kata_2021_PostmodernAntiVax decided to study specifically anti-vaccination misinformation because the current scientific consensus details one stance as true information and another as misinformation. In this study, they analyzed two aspects of vaccine misinformation on the internet; the frequency of pro- vs anti-vaccination websites on a standard Canadian and American Google search, and the content and themes that are presented in each anti-vaccination site to convince readers that vaccines should not be trusted.

The first finding that @Kata_2021_PostmodernAntiVax shared was the result of an analysis of hundreds of Google results from both Canada and America in May of 2009. A search for the word "vaccine" resulted in anti-vaccination websites 19% of the time in Canada and 25% of the time in America. Results from the word "vaccination" were far more disparate, as Canadian Google found anti-vaccination websites 17% of the time and American Google found them 71% of the time. A search for "immunisation OR immunization" resulted in pro-vaccination websites 100% of the time in both countries. The researchers specifically highlighted the search for "vaccination" in America, as more than two-thirds of the results were links to websites that advocated against vaccines.

After identifying anti-vaccination websites, @Kata_2021_PostmodernAntiVax manually identified themes present in each of them. In this analysis, the researchers found that the most common attributes highlighted by websites to advocate against vaccines are safety and effectiveness, conspiracies and the search for truth, and investigation of the contents of vaccines. Religion was mentioned about half of the time, and most of the websites shared information about alternative medicines. These results were summarized by the researchers to create a framework for vaccine hesitancy which includes three primary themes $-$ the belief of alternative health models, encouragement for parental control over children, and distrust of people who claim to be experts [@Kata_2021_PostmodernAntiVax]. The researchers claim that if this framework can be well enough understood, then education about vaccines can be more effective.

## Establishment of Consensus

Suppose that for each controversial topic, an algorithm or a set of people could generate a number that represented the "consensus" of its public or scientific understanding. If this were the case, social media sites could use these numbers to control the way that algorithms spread sources or as a part of the display for each source. This has the potential to stunt information that disagrees with scientific consensus, which may be an effective method for preventing misinformation.

@Plaza_2019_ConsensusAlgorithmsMedicalMisinformation performed a set of case studies to identify the origins of medical misinformation. In doing so, they discovered that many movements encouraging a deviation from scientific consensus begin with either a misunderstanding of scientific literature or with the publication of a paper that is not properly peer-reviewed. For example, a 2017 study concluded that its research methods were not rigorous enough to determine whether or not the presence of fluoride in tap water caused toxicity. Less than a year later, a cascade of sources citing this study claimed that fluoride was a neurotoxic chemical that had no place in the water supply. Twenty years earlier, in 1998, _The Lancet_ published a paper by Andrew Wakefield which found that specific vaccines caused autism in children. After this paper's publication, many independent scientists discovered critical errors in Wakefield's research methods and determined that the findings were not accurate. However, before _The Lancet_ retracted the paper, its findings had been viewed and accepted by a significant portion of the public. As a result, a large movement claiming that autism was caused by vaccinations had formed. The authors of this study concluded that one way to prevent situations like this from happening again is by establishing a metric of scientific consensus that could be assigned to each claim for the public to refer to.

An ideal metric to determine scientific consensus, according to @Plaza_2019_ConsensusAlgorithmsMedicalMisinformation, would allow for each claim to undergo review and scrutiny from a large set of unbiased experts. To solve this problem, they recommend the development of a distributed mechanism for the determination of medical scientific consensus. One mechanism that may be used to consistently verify consensus is the implementation of blockchain technology (BCT). According to the authors, BCT would prevent this verification from being oppressive in terms of scientific freedom while it reduced the likelihood of invalid claims making their way outside of the community. If implemented correctly, BCT would associate each claim with a set of reviews which ultimately provided a numerical "validity score" displayed as a percentage. In simple terms, if a claim is said to have a validity of over 50% then it can be said that it has scientific consensus. A higher validity score represents a claim that is more likely to be referred to as scientific fact, but it is impossible for a claim to ever reach this point through this system.

No matter how close a claim is to achieving complete scientific consensus, it is important to refrain from considering it as fact. Paradigm shifts happen in science, and ideas that were heavily regarded as the truth are now known to be false. One example in which scientific consensus was incorrect in the field of medical information was the 1955 understanding of peptic ulcers. When researchers published a paper claiming that an infection called _Helicobacter pylori_ was the root cause of peptic ulcers, they were met with scrutiny and denial from the vast majority of the scientific community. However, after 30 years of consistent research in the field that showed the validity of the claim, scientific consensus slowly shifted in favor of the researchers. Because these claims circulated for multiple decades before scientific consensus was updated, it can be argued that many lives were lost unnecessarily. @Plaza_2019_ConsensusAlgorithmsMedicalMisinformation assert that BCT would prevent situations like this from taking as long as they did in the twentieth century by increasing the efficiency with which scientific consensus is updated. As effective as this technology may become once it is applied, it will not become useful until an effective method for gathering consensus is developed. This is why researchers such as @Resnick_2021_InformedCrowdsIdentifyMisinformation and @Roitero_2020_CrowdIdentifyMisinformationObjectively have focused on the idea that crowds of people could effectively identify misinformation.

Before organizing their study, @Resnick_2021_InformedCrowdsIdentifyMisinformation optimistically argued that, while individuals are not always reliable at identifying misinformation, it may be possible for groups of people to collectively and democratically determine whether an article or news source is misleading. Past research has shown that political biases do not significantly affect the average rater's ability to distinguish sources of mainstream news from those of hyperpartisan and fake news, but until this study, there was no similar analysis for individual articles. Thus, a study was organized in which the objective was to determine the correlation between the identification of misinformation by liberal voters, conservative voters, and journalists who have been trained in objectivity.

To perform this experiment, @Resnick_2021_InformedCrowdsIdentifyMisinformation began by hiring a group of untrained and ideologically balanced lay raters via Amazon's Mechanical Turk and 4 professional journalists who had completed certifications via a fellowship for mid-career journalists. The lay raters were each separated into one of nine groups; first, an initial screening identified each as either liberal, conservative, or moderate. Then each ideological group was given a set of articles and a condition under which to rate them based on believability. Based on these conditions, raters either judged articles after no additional research, after research that was self-guided or after that which was based on a given set of other ideologically balanced articles. Because the researchers were not qualified to label articles based on believability, they performed an analysis based on the correlation between the labels given by lay raters and those given by journalists.

Many of the results found by this study were aligned with those of previous research. Regardless of the amount of research, it was found that liberal and conservative voters were highly correlated in decisions about believability. However, conservative voters were less likely on average to agree with the decisions made by journalists. One statistical analysis found that 3.54 self-researching liberal voters were equivalent to a single journalist, while the equivalent number of conservative voters was 5.45. The limitations of the experiment did not allow for an "equivalency rating" with journalists that was higher than 18, and the conservative crowd exceeded this limit far more often than liberal voters did. Regardless of political affiliation, it was found that lay raters who did research themselves were more likely to agree with journalists than those who were given a politically balanced set of articles to read beforehand. The most evident result found by @Resnick_2021_InformedCrowdsIdentifyMisinformation was that raters who have performed research on the subject before rating the believability of an article are far less likely to disagree with journalists and more likely to agree with the opposing political affiliation.

A year before @Resnick_2021_InformedCrowdsIdentifyMisinformation performed their study, @Roitero_2020_CrowdIdentifyMisinformationObjectively asked a similar question about political statements. Other researchers in the past have produced sets of data containing thousands of statements made by politicians and their objective "truthfulness" based on expert research and numerical proof. After performing post-processing on two datasets, `PolitiFact` and `ABC`, they had gathered over 13,000 short political statements, the politicians who had said them, and the year in which they had been stated. Using Amazon's Mechanical Turk service to recruit workers, @Roitero_2020_CrowdIdentifyMisinformationObjectively gave a subset of these political statements to thousands of American laypeople. After each worker had a set of statements, they were appointed to rate them based on one of three scales; $S_3$ contains three levels (`negative`, `in-between`, and `positive`), $S_6$ contains six levels (`lie`, `false`, `barely true`, `half true`, `mostly true`, and `true`), and $S_{100}$ contains 100 levels (effectively describing "percentage this statement that is correct").

After collecting data from about six hundred crowd workers from the United States, @Roitero_2020_CrowdIdentifyMisinformationObjectively performed an analysis to find the effectiveness of crowds in detecting truthfulness. While there was a low level of internal agreement between laypeople, it was found that on average, large groups of people are capable of labeling information as either true or false most of the time if they are allowed the opportunity to perform research on the topic. This study also found that, while people typically retrieve a source from the first page of search engine results, it can not be expected that they will always select from the very beginning of the page of results. In general, the workers selected via Mechanical Turk seemed to have put effort into finding a reliable source. They also found that the workers' political affiliation had little effect on their ability to detect the truth in statements, but their derived implicit political biases were shown to have some correlation with their ability to identify correctly.

Researchers @Pennycook_2019_CrowdsourcedJudgements were also interested in studying the effectiveness of crowdsourcing in order to distinguish between real and fake information. However, these researchers worked under the assumption that all articles from each news source are close to the same amount of quality in terms of trustworthiness. If this is the case, then limiting access to untrustworthy news sites on social media could be an effective method for preventing the spread of misinformation. The researchers in this study compared the trust in various mainstream, hyper-partisan, and blatantly fake news sources by democrats, republicans, and professional fact-checkers. This study was performed in a similar way to that of @Roitero_2020_CrowdIdentifyMisinformationObjectively and @Resnick_2021_InformedCrowdsIdentifyMisinformation; about one thousand individuals were hired through Amazon's Mechanical Turk and from a similar service called Lucid, and demographic information was gathered about each person before they evaluated the trust that they had in each of the selected news sources.

The results retrieved by @Pennycook_2019_CrowdsourcedJudgements to determine the trustworthiness of news sources were significantly more clear-cut than the crowdsourced judgments that determined the trustworthiness of articles [@Resnick_2021_InformedCrowdsIdentifyMisinformation] or political statements [@Roitero_2020_CrowdIdentifyMisinformationObjectively]. In this study, they found a remarkably high correlation between laypeople and fact-checkers in terms of distinguishing trustworthy news outlets from those that are untrustworthy. The researchers claim that, based on the results they found, social media outlets could potentially reduce the spread of misinformation by incorporating trust ratings into their source rating algorithms.

While grouping all laypeople into a single group was sufficient for ranking news outlets based on trustworthiness for @Pennycook_2019_CrowdsourcedJudgements, they did find a fairly significant difference between the ratings by democrats and republicans. In general, democrats provided ratings that were closer to those of fact-checkers than republicans did. These results were affected by more than simply the correlation between political leaning in participants and sources, as the researchers performed an analysis in which they controlled for this. It was found that these results are significant because of republicans' distrust of mainstream media, and not either party's trust in hyperpartisan or fake news sites.

## Detection of Misinformation with Machine Learning

The reality of the social media landscape is that it allows for extremely large volumes of individual authors to post about a single topic in a very short time. Therefore, the manual deletion of any reasonable amount of misinformed posts or articles could easily be drowned out by the sheer volume of posts about the topic. This is why a body of researchers like @Yu_2019_convolutionalMisinformationIdentification and @Du_2021_DetectVaccineMisinfoWithML have opted instead to propose alternative approaches for the detection and flagging of misinformed social media posts using machine learning.

Because they were working with algorithms instead of manual detection to detect misinformation, researchers @Yu_2019_convolutionalMisinformationIdentification have been able to take a much more holistic approach than studies that studied single articles, posts, or quotes. These researchers modeled a machine learning algorithm based not only on just the text contained within isolated social media posts but also on a large number of similar posts that were published in the same time frame and on the replies for each of them. Their technical publication details the pieces of an `ACAMI` convolutional neural network that work together in order to determine falsehoods trending on Twitter. The goal of this model is to find "events" that are posted about frequently on the platform and to identify the posts that belong to each of these events. After the events have been identified, the algorithm can generate a representation of each of them which contains both misinformation and true information. Using this abstracted representation, the algorithm is capable of separating posts about each event into groups that may contain misinformation. Finally, the `ACAMI` network has been trained to search through replies to the posts in each group in order to identify whether there are large groups of people who are skeptical about the posts within them.

Because language is constantly evolving and cultural norms are changing, it is nearly impossible to detect all types of misinformation through analysis of language. Social media also has other unique features in its language which are constantly changing and evolving, such as slang, incomplete sentences, and misspellings [@Du_2021_DetectVaccineMisinfoWithML]. This is why @Du_2021_DetectVaccineMisinfoWithML have decided to train neural networks and machine learning models to predict _only_ misinformation about the Human papillomavirus (HPV) vaccine in a specific social media platform. Reddit provides a service called Pushshift which allows people to gather a large number of posts about a given topic without finding them manually, so the researchers were able to gather over 28,000 examples that were posted within 10 years. The researchers manually pored through 2,200 of these examples and determined whether they communicated primarily misinformation or true information before using them to train a series of machine learning models.

After the models had been trained by the manually labeled posts, @Du_2021_DetectVaccineMisinfoWithML applied them to the rest of the posts to acquire a subset of them which were statistically expected to be spreading misinformation. Using this method, they found 7,207 of the 28,121 posts (25.6%) to be misinformation. After they had identified these misinformed posts, they performed topic analysis and found 10 algorithmically identified topics, which they condensed into 7 using real-world understanding and intuition. These topics included adverse effects of vaccines, conspiracy theories, citations of unfounded studies, death and injury due to vaccines, aluminum in adjuvants, and vaccine-induced autism [@Du_2021_DetectVaccineMisinfoWithML].

Researchers @Du_2021_DetectVaccineMisinfoWithML hope that their unique approach at algorithmically detecting misinformation in social media posts can be applied within other platforms and for a wide variety of misinformed topics. While it is not reasonable to use algorithms like this in order to limit social media posts from spreading because of inherent biases, this method could potentially be used as a tool to flag posts before they are manually analyzed.

## Biases in Machine Learning Algorithms

As machine learning and computational intelligence have been integrated more into fields that had previously relied on human intelligence, it has become increasingly apparent that reliance on computer models without understanding often results in negative effects. @Gianfrancesco_2018_PotentialBiasesInMLHealth discuss the potential ramifications of this knowledge gap in the field of healthcare. One important consideration is that the term "machine learning" refers to a large body of algorithms that have a variety of strengths and weaknesses. For example, methods such as decision tree and market basket analysis provide associations between variables and significant correlations. Even though the statistical analysis is abstracted away from the people who use these methods, it is left to professionals to interpret the useful output and that which is obvious or not important. Because of the way these models are constructed they are unable to differentiate between correlation and causation, so it is up to the user to differentiate. On the other hand, many of the most sophisticated machine learning models like deep learning act as black boxes that simply take in a body of information and respond with a conclusion. Currently, these models are used in healthcare to predict in-hospital mortality and expected length of stay. There are other instances in which machine learning is used to identify anomalies in medical scans.

Most use cases for machine learning have benefited the healthcare system without significant negative effects, but @Gianfrancesco_2018_PotentialBiasesInMLHealth have highlighted the inequalities and biases that may arise if the algorithms are not properly understood. There is an argument to be made that excessive use of machine learning is likely to perpetuate or worsen existing inequalities. One reason for this is that larger amounts of data are usually correlated with higher accuracy in algorithms. It is known that hospitals have significantly larger volumes of information for people who have a higher social status, and fewer for marginalized groups. Most machine learning algorithms are excellent at identifying meaningful information without context but are unable to distinguish data that matters within the context of the field for which they are applied. For example, one algorithm found that the highest predictor of death during a hospital stay is the number of visits from friends and family; any hospital worker knows that this is simply a correlation and death is not caused by hospital visits.

The solution that is provided by @Gianfrancesco_2018_PotentialBiasesInMLHealth is excessive documentation for each type of machine learning algorithm that is designed to be read by laypeople. Without an understanding of how these algorithms work, people who are using a machine learning-based system are at risk of feeding the algorithms with biased data or of misinterpreting the results that they are given. It is also recommended that hospitals only apply machine learning to places where it is demonstrated that results will be clinically important. Overreliance on algorithms in areas where they do not provide meaningful results may degrade those areas of the health field, and this should not happen in excessive amounts.

## Non-algorithmic Spread of Misinformation

Limiting exposure to misinformation via a top-down approach in which algorithms detect and label misinformed posts, as @Yu_2019_convolutionalMisinformationIdentification and @Du_2021_DetectVaccineMisinfoWithML have proposed (and @Gianfrancesco_2018_PotentialBiasesInMLHealth critiqued), has the potential to be an effective method of reducing the spread of misinformation in social media. However, some ethical concerns may be associated with an approach that takes power away from the individuals who are posting content and puts it into the hands of algorithms and people who claim to be experts in "truth". No matter how well algorithms and social media platforms can detect misinformed posts, barring a totalitarian force in control of the internet there will never be a way to fully eliminate the spread of misinformation until there is a better understanding of why people share these posts in general. @Pennycook_2021_AttentionToAccuracy speculate in their study that users of social media do not prioritize accuracy before sharing a post on social media, and therefore often share articles and headlines that they have not fact-checked. To test this theory, they performed an experiment in which participants were prompted to answer questions about the accuracy of various studies as well as the likelihood that they would share it.

@Pennycook_2021_AttentionToAccuracy separated their study into three phases. In the first experiment, they tested their hypothesis that the correlation between perceived accuracy and the likelihood of sharing was not as strong as many would expect. To perform this experiment, the researchers surveyed a thousand individuals with a set of news headlines and the perceived accuracy of those headlines, as well as the likelihood that the person would share each post. After the survey was completed, the researchers gathered general demographic information that included political partisanship. From this experiment, they found that partisanship had much less of an effect on the perceived accuracy of a news headline than did the actual accuracy of the statement. However, the opposite effect was found when people were surveyed about shareability instead of accuracy; politically concordant headlines were 10% more likely to be shared than those which were considered true by participants. The results of this experiment show that there is less correlation between perception of accuracy and likelihood of sharing than is implied by other studies [@Pennycook_2021_AttentionToAccuracy].

The findings from the first experiment in this study were important in the evaluation of the data acquired by the second one. In the second experiment, a similar group of participants was asked about the accuracy of an article _before_ rating another one by shareability. Because of this priming about the accuracy, the researchers expected that participants would be thinking about the accuracy of the second article while they rated it on shareability. This expectation held, and the priming about accuracy had a significant effect on participants. The researchers found that participants in the treatment group were far less likely than the control to want to share inaccurate articles, and slightly more likely to share those which were accurate. It is important to note that these participants were not told that sharing accurate articles is important; instead, they were only nudged into the direction of _thinking_ about accuracy by rating a different article beforehand.

In the third portion of their study, @Pennycook_2021_AttentionToAccuracy put their results to the test by applying them to an experiment performed live on Twitter. The researchers reached out to thousands of people who had recently posted articles from unreliable sources hoping to prime them into thinking about accuracy for the future. In this intervention, they asked each responding user to rate a non-political headline on its accuracy and then watched how they acted in their next few posts. Surprisingly, they found that this intervention had a real effect on its participants. Relative to the baseline, there was a significant improvement in the quality of news sources shared within the next few posts. Because of these findings, @Pennycook_2021_AttentionToAccuracy suggested that, in the future, social media users should be subtly prompted to think about the accuracy of news sources throughout their social media experience. If this intervention is effective, the results of this experiment suggest that the spread of misinformation in social media will be significantly reduced.

# Rationale

Due to the vastness of the freedom that has been provided by the internet and modern media, it is unlikely that any totalitarian effort to control misinformation through identifying and eliminating it, either manually or algorithmically, will be successful. Therefore, social media sites and other outlets of large unregulated information must discover methods for discouraging unverified or misinformed posts in a natural, unobtrusive manner. One way to do this is by subtly nudging users in the direction of ensuring that they share accurate information instead of impactful headlines, as is shown to have been effective by @Pennycook_2021_AttentionToAccuracy. Any additional unobtrusive interventions that are shown to affect the frequency at which misinformation is shared have the potential to be very useful in the mitigation of misinformation propagation. This paper functions as a proposal for further research into the potential for intervention by way of bottlenecking the propagation of all information as it spreads throughout the media, in hope that this will result in a shift in the way that information spreads.

This experiment has been designed based on the hypothesis that modifying the speed of information propagation will result in some change in the likelihood that unverified or false posts on social media will overcome facts and scientifically understood truths as they spread throughout the platform. If reducing the spread of all information results in a significant reduction in the number of end-users who are presented with unverified news and posts, the hypothesis can be considered supported and further research can be developed in order to find ways that social media platforms can use the findings in their algorithms. Otherwise, the hypothesis will be rejected and data can be analyzed for any additional findings.

# Method and Design

The purpose of this study is to identify whether providing a bottleneck to the spread of _all_ posts that share information on a social media platform will disincentivize the spread of misinformation. Therefore, the primary function of this experiment is to modify the algorithms that a social media platform uses to select the set of posts that it shares with each user who is exploring content from unknown authors. After this modification is made, researchers will perform an analysis of the trends that occurred in the propagation of misinformation. 

## Selection of Social Media platform

In order for this experiment to find effective results, it must be conducted on the _entirety_ of a social media platform. This selected platform must contain the following qualities:

1. A primary way in which people interact with the platform is by viewing a feed of suggested posts from accounts to whom the user is not directly associated. The algorithms that will be modified in this experiment are those which determine the posts that are shown in this feed. Because this study is based on an analysis of the propagation of ideas, a large body of users must be primarily using this feature of the platform.
2. The platform must have built-in systems in place that separate posts that are designed to share information from those which are designed for entertainment or other purposes. Ideally, the algorithms which are used to suggest each of these types of posts can be controlled separately. This is important because most media platforms have a body of users who make their living designing posts that are effective under these algorithms, so the impact should be minimized as much as possible.
3. It must be possible for researchers to acquire permissions to query for posts on the platform at a large scale, in order to search for keywords and gather information across a time scale. Without this capability, it will be impossible to perform analysis before and after changes are made.

Of the major social media platforms in 2021, it can be argued that the one which most closely adheres to these qualifications is **TikTok**. The primary function of this platform is to allow users to present users with a feed of posts from a wide variety of other creators, including people who have been followed and liked as well as those who have never been seen before. While there is not a definitive flag that marks specific posts as news and others as entertainment, there are multiple systems in place which may be used to separate posts. The rest of this proposal will work under the assumption that after the algorithms have been modified, researchers will be permitted to access TikTok's database of posts in order to run statistical analysis.

## Modification of Algorithms

TikTok published a press release in 2020 detailing a general overview of the way that their algorithm currently works [@TikTok_2019_VideoRecommendations]. In this press release, the creators of TikTok shared with the public that their recommendation algorithm strategically injects the main `#ForYou` feed with videos that are unrelated to anything that a given user has seen before. There are sophisticated algorithms that decide which video to suggest to each user based on a large number of factors, including previous viewership and subscriptions as well as watch time on previous videos. After a post has been published, TikTok's algorithms gather information about the language that is used and the content of its video using thousands of variables. While none of these variables directly state whether each post is designed to communicate information or provide entertainment, some combination of them can be acquired (manually or algorithmically) which may be used to identify all posts directed at sharing information.

The purpose of this experiment is to identify whether bottlenecking the spread of all information-sharing posts will discourage misinformation from overtaking verified information in terms of likes and sustained viewership. For simplicity, this bottleneck may be generated by applying a hard numerical limit to the number of times that each post can be shown on any `#ForYou` stream within a certain block of time. The actual limit and timeframe should be decided based on numerical expectations shared by the team at TikTok, but ideally, it would restrict information-sharing posts so that they could spread linearly throughout the platform instead of exponentially with time.

## Propogation analysis

While this study is inherently quantitative, there are important qualitative elements that must be selected manually. The most important of these quantitative elements is a list of controversial topics that were seen to have emerged in the news within the time frame of the study. Each of these topics should encourage the presence of two or more separable "camps" of people who aim to share opposing views, where a proper subset of these views agree with the scientific and academic consensus while the others starkly disagree. Researchers are to identify a set of topics that emerged before the algorithm was changed and another set of topics that emerged afterward. After a list of topics have been identified, the following steps can be taken in order to collect information on the way that they had propagated throughout the platform:

1. Researchers identify a set of keywords or phrases which uniquely identify the topic at hand, and can be used to find all or most posts that agree _and_ disagree with the scientific consensus. Using this set of keywords, the researchers can query TikTok's database using the subtitles associated with each post in order to find as many posts about the topic as possible.
2. Poring through all of these posts individually to identify which of them are misinformed and which agree with scientific consensus is infeasible, because researchers would likely have to spend countless hours identifying millions of posts. Instead, researchers can identify the status of a sufficient number of posts to train a neural network for automatic identification in a similar way to the method that @Du_2021_DetectVaccineMisinfoWithML used in their study. While it is inevitable that the neural network will misclassify a portion of the posts, researchers can continue to identify posts manually until a certain threshold of accuracy is reached.
3. After the neural network has been trained enough to consistently identify posts with a high enough accuracy, it can be used to classify _all_ of the posts as either "informed" or "misinformed". Because each post is associated with a timestamp, it is possible to identify trends in each topic throughout its lifespan.

After data is collected, researchers will be left with a massive set of posts labeled as either "misinformation" or "true information" that are related to each of the manually selected topics. These sets of posts can be analyzed for patterns and trends. Because the goal of this experiment is to identify the frequency at which misinformation spreads through time, the primary analysis will identify the proportion of misinformed posts about a given topic during blocks of time throughout its lifespan.

## Expectations for Results

If the hypothesis is correct, the bottleneck that is applied to each post will, on average, allow for the body of users to identify unverified but impactful misinformation as false and give more attention to the posts that seem to be scientifically verifiable. The results of the study by @Pennycook_2021_AttentionToAccuracy have shown that most people are capable of identifying misinformation, but share and pay attention to it for reasons other than its value of truth. Therefore, it may be possible that verifiable information naturally stays respected within the platform while misinformation loses attention if both positions are allowed to propagate at the same rate. 

If the hypothesis is incorrect, the proportion of misinformation within the platform will have remained constant after the change has been made to the algorithm.

# Conclusion

Most research that has been published about misinformation is designed either as an analysis of its qualities or as an experiment that aims to disincentivize its spread. Logically, the research that performs analysis is designed to encourage further research into disincentivization. While the body of published research in the former category is fairly balanced in between the study of algorithms and the study of psychological factors that lead to misinformation, it seems as though an overwhelming majority of the publications which study disincentivization are researching psychological factors. To effectively reduce the spread of misinformation in social media, both psychological and technological factors must be considered. The experiment performed in this study is designed to acquire additional information about how algorithms may be modified to reduce the spread of misinformation. While the psychology of the platforms' users is considered, the primary focus is discovering underlying rules that encourage excessive propagation of misinformed news and information.

Most research about algorithmic detection of misinformation attempts to use neural networks or other machine learning technology to identify whether a set of posts or publishers is trustworthy. While this works in many contained environments, the dynamic and constantly changing sphere of social media is not a place where accuracy can stay consistent without excessive manual intervention. If instead, a modification for algorithms that spread information can be discovered that disincentivizes their spread, no further intervention would be required. If the hypothesis in this study is correct, the changes to the algorithms that social media platforms use may be applied directly in order to reduce misinformation in social media.

# Bibliography

```{=latex}
\begin{hangparas}{2em}{1}
```

<div id="refs"></div>

```{=latex}
\end{hangparas}
```

# Appendix

## TikTok Modification Request

This letter functions as a formal request for the development team at TikTok to modify the algorithms that are used to share posts in the `#ForYou` stream in a controlled experiment designed to evaluate the spread of misinformation. The general objective of this study is to determine whether slowing the propagation of _all_ information-sharing posts will disincentivize sustained viewership and support of misinformation. In order to do this, the `#ForYou` algorithm at TikTok must be modified to allow for a numerical limit to be applied which limits the number of times that each post can be selected to appear on any user's feed within a given block of time.

By agreeing to participate in this study, you grant permission to allow the researchers to view blocks of anonymized data about various controversial topics. This data may be acquired by employees at TikTok, or limited permissions may be granted for database access to researchers.